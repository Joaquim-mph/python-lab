{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf6a400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "import matplotlib.dates as mdates\n",
    "import datetime as dt\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Detect repo root: parent of ./notebooks\n",
    "repo = Path.cwd().resolve()\n",
    "if repo.name == \"notebooks\":\n",
    "    repo = repo.parent\n",
    "sys.path.append(str(repo))\n",
    "\n",
    "\n",
    "from src.ploting.styles import set_plot_style\n",
    "set_plot_style(\"prism_rain\")\n",
    "from src.parsing.runners import process_all_raw_recursive\n",
    "from src.warehouse.db_build import build_warehouse, build_timeline_dataset\n",
    "from src.parsing.runners import process_all_raw_recursive  # nombre según tu módulo\n",
    "\n",
    "from src.warehouse.builder import build_parquet_from_raw_parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0f11ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw->parquet] scan /home/mphstphls/python-lab/data/00_raw … csv=30600\n",
      "  … 200/30600 processed\n",
      "  … 400/30600 processed\n",
      "  … 600/30600 processed\n",
      "  … 800/30600 processed\n",
      "  … 1000/30600 processed\n",
      "  … 1200/30600 processed\n",
      "  … 1400/30600 processed\n",
      "  … 1600/30600 processed\n",
      "  … 1800/30600 processed\n",
      "  … 2000/30600 processed\n",
      "  … 2200/30600 processed\n",
      "  … 2400/30600 processed\n",
      "  … 2600/30600 processed\n",
      "  … 2800/30600 processed\n",
      "  … 3000/30600 processed\n",
      "  … 3200/30600 processed\n",
      "  … 3400/30600 processed\n",
      "  … 3600/30600 processed\n",
      "  … 3800/30600 processed\n",
      "  … 4000/30600 processed\n",
      "  … 4200/30600 processed\n",
      "  … 4400/30600 processed\n",
      "  … 4600/30600 processed\n",
      "  … 4800/30600 processed\n",
      "  … 5000/30600 processed\n",
      "  … 5200/30600 processed\n",
      "  … 5400/30600 processed\n",
      "  … 5600/30600 processed\n",
      "  … 5800/30600 processed\n",
      "  … 6000/30600 processed\n",
      "  … 6200/30600 processed\n",
      "  … 6400/30600 processed\n",
      "  … 6600/30600 processed\n",
      "  … 6800/30600 processed\n",
      "  … 7000/30600 processed\n",
      "  … 7200/30600 processed\n",
      "  … 7400/30600 processed\n",
      "  … 7600/30600 processed\n",
      "  … 7800/30600 processed\n",
      "  … 8000/30600 processed\n",
      "  … 8200/30600 processed\n",
      "  … 8400/30600 processed\n",
      "  … 8600/30600 processed\n",
      "  … 8800/30600 processed\n",
      "  … 9000/30600 processed\n",
      "  … 9200/30600 processed\n",
      "  … 9400/30600 processed\n",
      "  … 9600/30600 processed\n",
      "  … 9800/30600 processed\n",
      "  … 10000/30600 processed\n",
      "  … 10200/30600 processed\n",
      "  … 10400/30600 processed\n",
      "  … 10600/30600 processed\n",
      "  … 10800/30600 processed\n",
      "  … 11000/30600 processed\n",
      "  … 11200/30600 processed\n",
      "  … 11400/30600 processed\n",
      "  … 11600/30600 processed\n",
      "  … 11800/30600 processed\n",
      "  … 12000/30600 processed\n",
      "  … 12200/30600 processed\n",
      "  … 12400/30600 processed\n",
      "  … 12600/30600 processed\n",
      "  … 12800/30600 processed\n",
      "  … 13000/30600 processed\n",
      "  … 13200/30600 processed\n",
      "  … 13400/30600 processed\n",
      "  … 13600/30600 processed\n",
      "  … 13800/30600 processed\n",
      "  … 14000/30600 processed\n",
      "  … 14200/30600 processed\n",
      "  … 14400/30600 processed\n",
      "  … 14600/30600 processed\n",
      "  … 14800/30600 processed\n",
      "  … 15000/30600 processed\n",
      "  … 15200/30600 processed\n",
      "  … 15400/30600 processed\n",
      "  … 15600/30600 processed\n",
      "  … 15800/30600 processed\n",
      "  … 16000/30600 processed\n",
      "  … 16200/30600 processed\n",
      "  … 16400/30600 processed\n",
      "  … 16600/30600 processed\n",
      "  … 16800/30600 processed\n",
      "  … 17000/30600 processed\n",
      "  … 17200/30600 processed\n",
      "  … 17400/30600 processed\n",
      "  … 17600/30600 processed\n",
      "[fail] /home/mphstphls/python-lab/data/00_raw/LabComputer1/data/2025-08-11/IVg2025-08-11_7.csv → /home/mphstphls/python-lab/data/02_warehouse/raw_measurements/day_id=1980-01-01/proc=IVg/computer=LabComputer1/part-ea36c65d01165f67.parquet: empty CSV\n",
      "\n",
      "This error occurred with the following context stack:\n",
      "\t[1] 'csv scan'\n",
      "\t[2] 'with_columns'\n",
      "\t[3] 'sink'\n",
      "\n",
      "[fail] /home/mphstphls/python-lab/data/00_raw/LabComputer1/data/2025-08-11/IVg2025-08-11_10.csv → /home/mphstphls/python-lab/data/02_warehouse/raw_measurements/day_id=1980-01-01/proc=IVg/computer=LabComputer1/part-db7452d124e64340.parquet: empty CSV\n",
      "\n",
      "This error occurred with the following context stack:\n",
      "\t[1] 'csv scan'\n",
      "\t[2] 'with_columns'\n",
      "\t[3] 'sink'\n",
      "\n",
      "[fail] /home/mphstphls/python-lab/data/00_raw/LabComputer1/data/2025-08-11/Tt2025-08-11_2.csv → /home/mphstphls/python-lab/data/02_warehouse/raw_measurements/day_id=1980-01-01/proc=unknown/computer=LabComputer1/part-0143dfff6080bd0d.parquet: empty CSV\n",
      "\n",
      "This error occurred with the following context stack:\n",
      "\t[1] 'csv scan'\n",
      "\t[2] 'with_columns'\n",
      "\t[3] 'sink'\n",
      "\n",
      "[fail] /home/mphstphls/python-lab/data/00_raw/LabComputer1/data/2025-08-11/IVg2025-08-11_9.csv → /home/mphstphls/python-lab/data/02_warehouse/raw_measurements/day_id=1980-01-01/proc=IVg/computer=LabComputer1/part-8b3570a2bb055bda.parquet: empty CSV\n",
      "\n",
      "This error occurred with the following context stack:\n",
      "\t[1] 'csv scan'\n",
      "\t[2] 'with_columns'\n",
      "\t[3] 'sink'\n",
      "\n",
      "[fail] /home/mphstphls/python-lab/data/00_raw/LabComputer1/data/2025-08-11/Tt2025-08-11_3.csv → /home/mphstphls/python-lab/data/02_warehouse/raw_measurements/day_id=1980-01-01/proc=unknown/computer=LabComputer1/part-25f54ce287d684da.parquet: empty CSV\n",
      "\n",
      "This error occurred with the following context stack:\n",
      "\t[1] 'csv scan'\n",
      "\t[2] 'with_columns'\n",
      "\t[3] 'sink'\n",
      "\n",
      "[fail] /home/mphstphls/python-lab/data/00_raw/LabComputer1/data/2025-08-11/IVg2025-08-11_11.csv → /home/mphstphls/python-lab/data/02_warehouse/raw_measurements/day_id=1980-01-01/proc=IVg/computer=LabComputer1/part-7a798842519bb232.parquet: empty CSV\n",
      "\n",
      "This error occurred with the following context stack:\n",
      "\t[1] 'csv scan'\n",
      "\t[2] 'with_columns'\n",
      "\t[3] 'sink'\n",
      "\n",
      "[fail] /home/mphstphls/python-lab/data/00_raw/LabComputer1/data/2025-08-11/IVg2025-08-11_8.csv → /home/mphstphls/python-lab/data/02_warehouse/raw_measurements/day_id=1980-01-01/proc=IVg/computer=LabComputer1/part-34f14e5b96e5fcf3.parquet: empty CSV\n",
      "\n",
      "This error occurred with the following context stack:\n",
      "\t[1] 'csv scan'\n",
      "\t[2] 'with_columns'\n",
      "\t[3] 'sink'\n",
      "\n",
      "  … 17800/30600 processed\n",
      "  … 18000/30600 processed\n",
      "  … 18200/30600 processed\n",
      "  … 18400/30600 processed\n",
      "  … 18600/30600 processed\n",
      "  … 18800/30600 processed\n",
      "  … 19000/30600 processed\n",
      "  … 19200/30600 processed\n",
      "  … 19400/30600 processed\n",
      "  … 19600/30600 processed\n",
      "  … 19800/30600 processed\n",
      "  … 20000/30600 processed\n",
      "  … 20200/30600 processed\n",
      "  … 20400/30600 processed\n",
      "  … 20600/30600 processed\n",
      "  … 20800/30600 processed\n",
      "  … 21000/30600 processed\n",
      "  … 21200/30600 processed\n",
      "  … 21400/30600 processed\n",
      "  … 21600/30600 processed\n",
      "  … 21800/30600 processed\n",
      "  … 22000/30600 processed\n",
      "  … 22200/30600 processed\n",
      "  … 22400/30600 processed\n",
      "  … 22600/30600 processed\n",
      "  … 22800/30600 processed\n",
      "  … 23000/30600 processed\n",
      "  … 23200/30600 processed\n",
      "  … 23400/30600 processed\n",
      "  … 23600/30600 processed\n",
      "  … 23800/30600 processed\n",
      "  … 24000/30600 processed\n",
      "  … 24200/30600 processed\n",
      "  … 24400/30600 processed\n",
      "  … 24600/30600 processed\n",
      "  … 24800/30600 processed\n",
      "  … 25000/30600 processed\n",
      "  … 25200/30600 processed\n",
      "  … 25400/30600 processed\n",
      "  … 25600/30600 processed\n",
      "  … 25800/30600 processed\n",
      "  … 26000/30600 processed\n",
      "  … 26200/30600 processed\n",
      "  … 26400/30600 processed\n",
      "  … 26600/30600 processed\n",
      "  … 26800/30600 processed\n",
      "  … 27000/30600 processed\n",
      "  … 27200/30600 processed\n",
      "  … 27400/30600 processed\n",
      "  … 27600/30600 processed\n",
      "  … 27800/30600 processed\n",
      "  … 28000/30600 processed\n",
      "  … 28200/30600 processed\n",
      "  … 28400/30600 processed\n",
      "  … 28600/30600 processed\n",
      "  … 28800/30600 processed\n",
      "  … 29000/30600 processed\n",
      "  … 29200/30600 processed\n",
      "  … 29400/30600 processed\n",
      "  … 29600/30600 processed\n",
      "  … 29800/30600 processed\n",
      "  … 30000/30600 processed\n",
      "  … 30200/30600 processed\n",
      "  … 30400/30600 processed\n",
      "  … 30600/30600 processed\n",
      "[ok] wrote /home/mphstphls/python-lab/data/02_warehouse/experiments_index.parquet rows=30593\n",
      "[done] written parts=30593\n",
      "{'written_parts': 30593, 'dataset_path': '/home/mphstphls/python-lab/data/02_warehouse/raw_measurements', 'index_path': '/home/mphstphls/python-lab/data/02_warehouse/experiments_index.parquet'}\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"POLARS_MAX_THREADS\"] = \"10\"  \n",
    "\n",
    "\n",
    "out = build_parquet_from_raw_parallel(\n",
    "    raw_root       = repo / \"data\" / \"00_raw\",\n",
    "    out_root       = repo / \"data\" / \"02_warehouse\",\n",
    "    procedures_yaml= repo / \"config\" / \"procedures.yml\",  # uses your YAML\n",
    "    overwrite      = True,\n",
    "    verbose        = True,\n",
    "    max_workers    = 8,   # tune for your disk/CPU\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8d8ddcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ComputeError",
     "evalue": "expected at least 1 source\n\nThis error occurred with the following context stack:\n\t[1] 'parquet scan'\n\t[2] 'group_by'\n\t[3] 'select'\n\t[4] 'sink'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mComputeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpolars\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpl\u001b[39;00m\n\u001b[32m      4\u001b[39m lf = pl.scan_parquet(\u001b[33m\"\u001b[39m\u001b[33mdata/02_warehouse/raw_measurements/**/part-*.parquet\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m counts = (\n\u001b[32m      7\u001b[39m     \u001b[43mlf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroup_by_dynamic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstart_dt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevery\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1d\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# group by day\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m      \u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m      \u001b[49m\u001b[43m.\u001b[49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstart_dt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43m      \u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Convert columns to lists\u001b[39;00m\n\u001b[32m     14\u001b[39m days = counts[\u001b[33m\"\u001b[39m\u001b[33mstart_dt\u001b[39m\u001b[33m\"\u001b[39m].to_list()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python-lab/.venv/lib64/python3.13/site-packages/polars/_utils/deprecation.py:97\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python-lab/.venv/lib64/python3.13/site-packages/polars/lazyframe/opt_flags.py:330\u001b[39m, in \u001b[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m         optflags = cb(optflags, kwargs.pop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[32m    329\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moptimizations\u001b[39m\u001b[33m\"\u001b[39m] = optflags\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python-lab/.venv/lib64/python3.13/site-packages/polars/lazyframe/frame.py:2407\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[39m\n\u001b[32m   2405\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2406\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2407\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mComputeError\u001b[39m: expected at least 1 source\n\nThis error occurred with the following context stack:\n\t[1] 'parquet scan'\n\t[2] 'group_by'\n\t[3] 'select'\n\t[4] 'sink'\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "\n",
    "lf = pl.scan_parquet(\"data/02_warehouse/raw_measurements/**/part-*.parquet\")\n",
    "\n",
    "counts = (\n",
    "    lf.group_by_dynamic(\"start_dt\", every=\"1d\")   # group by day\n",
    "      .agg(pl.len().alias(\"n\"))\n",
    "      .sort(\"start_dt\")\n",
    "      .collect()\n",
    ")\n",
    "\n",
    "# Convert columns to lists\n",
    "days = counts[\"start_dt\"].to_list()\n",
    "vals = counts[\"n\"].to_list()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.bar(days, vals, width=0.8)\n",
    "ax.set_title(\"Experiments per day\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "fig.autofmt_xdate()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
